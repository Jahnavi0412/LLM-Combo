{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3deb6c5fbd524986b5419943d45e99b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2eb94c33cdb54e87bfe7966914ad7782","IPY_MODEL_4b99edae90084cf0b48b7e1d75767cdd","IPY_MODEL_0903086b8cbe430b8f1204bc3c2d6d97"],"layout":"IPY_MODEL_b8c31b12b50e494f8db0e57d5ce78d13"}},"2eb94c33cdb54e87bfe7966914ad7782":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0230452da9a4e248fe66a2080d6b0de","placeholder":"​","style":"IPY_MODEL_3e38a9bfaff94723ac484c3dd660c96b","value":"themes/theme_schema@0.0.4.json: 100%"}},"4b99edae90084cf0b48b7e1d75767cdd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4af457cab4a540c0809452c34684a0db","max":12639,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c2b5ca0d8ed9451ca322f19a2d34977d","value":12639}},"0903086b8cbe430b8f1204bc3c2d6d97":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f28426704ca461ea9075ecf157eac01","placeholder":"​","style":"IPY_MODEL_9cf578365d9e4baf977e5396489075df","value":" 12.6k/12.6k [00:00&lt;00:00, 418kB/s]"}},"b8c31b12b50e494f8db0e57d5ce78d13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0230452da9a4e248fe66a2080d6b0de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e38a9bfaff94723ac484c3dd660c96b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4af457cab4a540c0809452c34684a0db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2b5ca0d8ed9451ca322f19a2d34977d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1f28426704ca461ea9075ecf157eac01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cf578365d9e4baf977e5396489075df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0NrtKZdrXBqr","outputId":"feb9ad0f-1f0e-466c-a0d6-10f43ebafa07"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gradio\n","  Downloading gradio-5.2.1-py3-none-any.whl.metadata (16 kB)\n","Collecting aiofiles<24.0,>=22.0 (from gradio)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Collecting fastapi<1.0,>=0.115.2 (from gradio)\n","  Downloading fastapi-0.115.2-py3-none-any.whl.metadata (27 kB)\n","Collecting ffmpy (from gradio)\n","  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n","INFO: pip is looking at multiple versions of gradio to determine which version is compatible with other requirements. This could take a while.\n","Collecting gradio\n","  Downloading gradio-5.1.0-py3-none-any.whl.metadata (15 kB)\n","Collecting gradio-client==1.4.0 (from gradio)\n","  Downloading gradio_client-1.4.0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting httpx>=0.24.1 (from gradio)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Collecting huggingface-hub>=0.25.1 (from gradio)\n","  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n","Collecting markupsafe~=2.0 (from gradio)\n","  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n","Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n","Collecting orjson~=3.0 (from gradio)\n","  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m548.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n","Collecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting python-multipart>=0.0.9 (from gradio)\n","  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n","Collecting ruff>=0.2.2 (from gradio)\n","  Downloading ruff-0.7.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n","Collecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting tomlkit==0.12.0 (from gradio)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n","Collecting uvicorn>=0.14.0 (from gradio)\n","  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.0->gradio) (2024.6.1)\n","Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.0->gradio)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n","Collecting starlette<0.41.0,>=0.37.2 (from fastapi<1.0,>=0.115.2->gradio)\n","  Downloading starlette-0.40.0-py3-none-any.whl.metadata (6.0 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n","Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n","  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","Downloading gradio-5.1.0-py3-none-any.whl (42.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio_client-1.4.0-py3-none-any.whl (319 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Downloading fastapi-0.115.2-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.6/436.6 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n","Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n","Downloading ruff-0.7.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n","Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading starlette-0.40.0-py3-none-any.whl (73 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, markupsafe, h11, ffmpy, aiofiles, uvicorn, starlette, huggingface-hub, httpcore, httpx, fastapi, gradio-client, gradio\n","  Attempting uninstall: markupsafe\n","    Found existing installation: MarkupSafe 3.0.1\n","    Uninstalling MarkupSafe-3.0.1:\n","      Successfully uninstalled MarkupSafe-3.0.1\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.24.7\n","    Uninstalling huggingface-hub-0.24.7:\n","      Successfully uninstalled huggingface-hub-0.24.7\n","Successfully installed aiofiles-23.2.1 fastapi-0.115.2 ffmpy-0.4.0 gradio-5.1.0 gradio-client-1.4.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 huggingface-hub-0.25.2 markupsafe-2.1.5 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.12 ruff-0.7.0 semantic-version-2.10.0 starlette-0.40.0 tomlkit-0.12.0 uvicorn-0.32.0 websockets-12.0\n"]}],"source":["!pip install --upgrade gradio\n"]},{"cell_type":"code","source":["!pip install litellm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w2eergvMz8ZT","outputId":"2e0c49bf-d99b-4340-ed4a-94f3a759ae01"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting litellm\n","  Downloading litellm-1.49.6-py3-none-any.whl.metadata (32 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from litellm) (3.10.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from litellm) (8.1.7)\n","Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (8.5.0)\n","Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm) (3.1.4)\n","Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (4.23.0)\n","Collecting openai>=1.51.0 (from litellm)\n","  Downloading openai-1.51.2-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (2.9.2)\n","Collecting python-dotenv>=0.2.0 (from litellm)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (2.32.3)\n","Collecting tiktoken>=0.7.0 (from litellm)\n","  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm) (0.19.1)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm) (3.20.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm) (2.1.5)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (24.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.20.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.51.0->litellm) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.51.0->litellm) (1.7.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.51.0->litellm) (0.27.2)\n","Collecting jiter<1,>=0.4.0 (from openai>=1.51.0->litellm)\n","  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.51.0->litellm) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.51.0->litellm) (4.66.5)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.51.0->litellm) (4.12.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->litellm) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->litellm) (2.23.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (2024.8.30)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.7.0->litellm) (2024.9.11)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (1.3.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (1.14.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (4.0.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->litellm) (0.25.2)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.51.0->litellm) (1.2.2)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.51.0->litellm) (1.0.6)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.51.0->litellm) (0.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2024.6.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->litellm) (0.2.0)\n","Downloading litellm-1.49.6-py3-none-any.whl (6.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openai-1.51.2-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-dotenv, jiter, tiktoken, openai, litellm\n","Successfully installed jiter-0.6.1 litellm-1.49.6 openai-1.51.2 python-dotenv-1.0.1 tiktoken-0.8.0\n"]}]},{"cell_type":"code","source":["import gradio as gr\n","from litellm import completion\n","from google.colab import userdata"],"metadata":{"id":"66qi2s1dz6fO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SecretKeyRetriever:\n","    \"\"\"\n","    Retrieves secret keys based on the given model.\n","    \"\"\"\n","\n","    MODEL_SECRET_MAPPING = {\n","        \"gemini/gemini-pro\": \"GEMINI_API_KEY\",\n","        \"groq/llm-34b-chat\": \"GROQ_API_KEY\",\n","        \"groq/gemma-7b-it\": \"GROQ_API_KEY\",\n","        \"groq/llama3-70b-8192\": \"GROQ_API_KEY\",\n","        \"groq/llama3-8b-8192\": \"GROQ_API_KEY\",\n","        \"groq/mixtral-8x7b-32768\": \"GROQ_API_KEY\"\n","        # Add more model-secret mappings as needed\n","    }\n","\n","    @staticmethod\n","    def get_secret_key(model):\n","        \"\"\"\n","        Returns the secret key associated with the given model.\n","\n","        Args:\n","            model (str): The name of the model.\n","\n","        Returns:\n","            str: The secret key from Colab secrets, or None if not found.\n","        \"\"\"\n","        secret_name = SecretKeyRetriever.MODEL_SECRET_MAPPING.get(model)\n","        if secret_name:\n","            return userdata.get(secret_name)\n","        return None"],"metadata":{"id":"zLS3Mm0n1Ps3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_llm(prompt, model_name, top_p=0.9, top_k=40, temperature=0.7, max_output_tokens=50):\n","    \"\"\"\n","    Receives prompt and model name, retrieves the API key from secrets,\n","    and calls the correct model using litellm.\n","    \"\"\"\n","\n","    # Get API key using SecretKeyRetriever\n","    api_key = SecretKeyRetriever.get_secret_key(model_name)\n","\n","    try:\n","        # Conditionally include top_k for Gemini\n","        kwargs = {\n","            \"model\": model_name,\n","            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n","            \"api_key\": api_key,\n","            \"top_p\": top_p,\n","            \"temperature\": temperature,\n","            \"max_tokens\": int(max_output_tokens)\n","        }\n","        if model_name == \"gemini/gemini-pro\":\n","            kwargs[\"top_k\"] = top_k\n","\n","        # Call litellm completion with specified model, prompt, and parameters\n","        response = completion(**kwargs)\n","        return response.choices[0].message.content  # Extract and return the generated text\n","    except Exception as e:\n","        return f\"Error: {e}\"\n"],"metadata":{"id":"SHNGP4vg2MX7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def llm_function(model, p_value, k_value, temperature, max_output_tokens, prompt, append_mode, current_output):\n","    # Simulated response generation based on input parameters\n","    output_text = f\"Model: {model}\\nPrompt: {prompt}\\nP-Value: {p_value}\\nK-Value: {k_value}\\nTemperature: {temperature}\\nMax Output Tokens: {max_output_tokens}\\n\"\n","\n","    # Simulated LLM response (this would be the actual LLM's output in a real case)\n","    generated_output = run_llm(prompt, model, p_value, k_value, temperature, max_output_tokens)#f\"Generated response for the prompt '{prompt}'\"\n","\n","    # Limit the generated response by max_output_tokens (simulated by truncating the response for demonstration)\n","    output_words = generated_output.split()[:int(max_output_tokens)]\n","    generated_output = \" \".join(output_words)\n","\n","    # Include token counts in the output\n","    output_text += f\"Input Tokens: {len(prompt.split())}, Output Tokens: {len(output_words)}\\n\\n\"\n","    output_text += f\"Generated Output: {generated_output}\"\n","\n","    # Append or erase output based on user choice\n","    if append_mode == \"Append\":\n","        final_output = current_output + \"\\n\\n\" + output_text\n","    else:\n","        final_output = output_text\n","\n","    return final_output\n","\n","my_theme = gr.Theme.from_hub(\"gstaff/xkcd\")\n","\n","# Define the Gradio app\n","with gr.Blocks(theme=my_theme) as demo:\n","    # Title of the app\n","    gr.Markdown(\"## LLM Playground\", elem_id=\"title\")\n","\n","    # Creating a row with two sliders per row and placing the prompt as the second artifact\n","    with gr.Row():\n","        with gr.Column(scale=1):\n","\n","            # LLM model dropdown with explanation on hover\n","            llm_dropdown = gr.Dropdown(\n","                [\"gemini/gemini-1.5-pro\",\"groq/gemma-7b-it\", \"groq/llama3-70b-8192\", \"groq/llama3-8b-8192\", \"groq/mixtral-8x7b-32768\"],\n","                label=\"Select LLM Model\",\n","                value=\"gemini/gemini-1.5-pro\",\n","                #info=\"Choose the AI model to use. Each model has different strengths, like answering technical questions or being creative.\"\n","            )\n","\n","            # Prompt input box (now second artifact in left column)\n","            prompt_input = gr.Textbox(\n","                label=\"Enter Prompt\",\n","                placeholder=\"Type your prompt here...\",\n","                lines=10,\n","                #info=\"Enter the prompt you want the AI to respond to.\"\n","            )\n","\n","            # Submit button with a custom CSS ID to change its background color\n","            submit_button = gr.Button(\"Generate\", elem_id=\"custom-button\")\n","\n","            gr.Markdown(\"### LLM Parameters\")\n","\n","            # First row: P-value and K-value sliders\n","            with gr.Row():\n","                p_value_slider = gr.Slider(\n","                    minimum=0.0, maximum=1.0, value=0.9, step=0.1,\n","                    label=\"Top-P\",\n","                    info=\"Controls the randomness in responses: lower values (near 0.0) make output focused, while higher values (near 1.0) boost creativity.\"\n","                )\n","                k_value_slider = gr.Slider(\n","                    minimum=0, maximum=100, value=40, step=1,\n","                    label=\"Top-K\",\n","                    info=\"Limits the number of words the AI considers: lower values make responses predictable, while higher values add variety.\"\n","                )\n","\n","            # Second row: Temperature slider and Max Output Tokens textbox\n","            with gr.Row():\n","                temperature_slider = gr.Slider(\n","                    minimum=0.0, maximum=2.0, value=0.7, step=0.1,\n","                    label=\"Temperature\",\n","                    info=\"Adjusts the AI's creativity: lower values make responses conservative, while higher values add more variety.\")\n","                max_output_tokens_textbox = gr.Textbox(\n","                    label=\"Max Output Tokens\",\n","                    value=\"3000\",\n","                    info=\"Set the maximum number of tokens (words) in the generated response.\"\n","                )\n","\n","            # Append or overwrite option using a dropdown\n","            append_option = gr.Dropdown(\n","                choices=[\"Append\", \"Erase and Add\"],\n","                value=\"Erase and Add\",\n","                label=\"Select Output Mode\",\n","                info=\"Choose whether to append to the current output or replace it with new content.\"\n","            )\n","\n","        # Second column for output display\n","        with gr.Column(scale=1):\n","            gr.Markdown(\"### Output\")\n","\n","            output_box = gr.Markdown(label=\"LLM Output\")\n","\n","    # Connect the button click to the function\n","    submit_button.click(\n","        fn=llm_function,\n","        inputs=[llm_dropdown, p_value_slider, k_value_slider, temperature_slider, max_output_tokens_textbox, prompt_input, append_option, output_box],\n","        outputs=output_box\n","    )\n","\n","# Launch the Gradio app\n","demo.launch()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":762,"referenced_widgets":["3deb6c5fbd524986b5419943d45e99b6","2eb94c33cdb54e87bfe7966914ad7782","4b99edae90084cf0b48b7e1d75767cdd","0903086b8cbe430b8f1204bc3c2d6d97","b8c31b12b50e494f8db0e57d5ce78d13","b0230452da9a4e248fe66a2080d6b0de","3e38a9bfaff94723ac484c3dd660c96b","4af457cab4a540c0809452c34684a0db","c2b5ca0d8ed9451ca322f19a2d34977d","1f28426704ca461ea9075ecf157eac01","9cf578365d9e4baf977e5396489075df"]},"id":"RnaTICRtmAMX","outputId":"a56e6391-4e9d-491b-f448-81a09b949a00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:90: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["themes/theme_schema@0.0.4.json:   0%|          | 0.00/12.6k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3deb6c5fbd524986b5419943d45e99b6"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://9d250c3d02e02857aa.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://9d250c3d02e02857aa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":6}]}]}